<!DOCTYPE html> <!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]--> <!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8"><![endif]--> <!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9"><![endif]--> <!--[if gt IE 8]><!--> <html class="no-js"><!--<![endif]--> <head> <meta charset="UTF-8"> <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"> <meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"> <title>[NLP] BERT란 무엇인가? &#8211; Jaeseok Kim blog</title> <meta name="description" content="I'm a Machine Learning enginear."> <meta name="keywords" content="NLP, BERT, Transfer-learning, Transformer"> <!-- Twitter Cards --> <meta name="twitter:card" content="summary"> <meta name="twitter:image" content="https://moongmoong.github.io/assets/img/tf_logo.png"> <meta name="twitter:title" content="[NLP] BERT란 무엇인가?"> <meta name="twitter:description" content="BERT에 대한 개념적인 설명"> <meta name="twitter:site" content="@username"> <meta name="twitter:creator" content="@username"> <!-- Open Graph --> <meta property="og:locale" content="en_US"> <meta property="og:type" content="article"> <meta property="og:title" content="[NLP] BERT란 무엇인가?"> <meta property="og:description" content="BERT에 대한 개념적인 설명"> <meta property="og:url" content="https://moongmoong.github.io/Bert/"> <meta property="og:site_name" content="Jaeseok Kim blog"> <meta property="og:image" content="https://moongmoong.github.io/assets/img/tf_logo.png"> <link rel="canonical" href="https://moongmoong.github.io/Bert/"> <link href="https://moongmoong.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Jaeseok Kim blog Feed"> <!-- Handheld --> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <!-- CSS --> <link rel="stylesheet" href="https://moongmoong.github.io/assets/css/main.css"> <!-- JS --> <script src="https://moongmoong.github.io/assets/js/modernizr-3.3.1.custom.min.js"></script> <!-- Favicons --> <link rel="apple-touch-icon" href="https://moongmoong.github.io/assets/img/favicons/apple-icon-precomposed.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://moongmoong.github.io/assets/img/favicons/apple-icon-72x72.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://moongmoong.github.io/assets/img/favicons/apple-icon-114x114.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://moongmoong.github.io/assets/img/favicons/apple-icon-144x144.png"> <link rel="shortcut icon" type="image/png" href="https://moongmoong.github.io/favicon.png" /> <link rel="shortcut icon" href="https://moongmoong.github.io/favicon.ico" /> <!-- Background Image --> <style type="text/css">body {background-image:url(https://moongmoong.github.io/assets/img/placeholder-city.jpg); background-repeat: no-repeat; background-size: cover; }</style> <!-- Post Feature Image --> </head> <body> <nav id="dl-menu" class="dl-menuwrapper" role="navigation"> <button class="dl-trigger">Open Menu</button> <ul class="dl-menu"> <li><a href="https://moongmoong.github.io/">Home</a></li> <li> <a href="#">About</a> <ul class="dl-submenu"> <li> <img src="https://moongmoong.github.io/assets/img/tf_logo.png" alt="Jaeseok Kim blog photo" class="author-photo"> <h4>Jaeseok Kim blog</h4> <p>I'm a Machine Learning enginear.</p> </li> <li><a href="https://moongmoong.github.io/about/"><span class="btn btn-inverse">Learn More</span></a></li> <li> <a href="http://twitter.com/username" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a> </li> <li> <a href="http://facebook.com/username" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-facebook-square"></i> Facebook</a> </li> <li> <a href="http://instagram.com/username" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-instagram"></i> Instagram</a> </li> <li> <a href="http://github.com/username" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-github"></i> Github</a> </li> <li> <a href="http://steamcommunity.com/id/username" target="_blank" rel="noopener noreferrer"><i class="fa fa-fw fa-steam-square"></i> Steam</a> </li> </ul><!-- /.dl-submenu --> </li> <li> <a href="#">Posts</a> <ul class="dl-submenu"> <li><a href="https://moongmoong.github.io/posts/">All Posts</a></li> <li><a href="https://moongmoong.github.io/tags/">All Tags</a></li> <li><a href="https://moongmoong.github.io/category/">All Category</a></li> </ul> </li> <li><a href="https://moongmoong.github.io/projects/" >Projects</a></li> </ul><!-- /.dl-menu --> </nav><!-- /.dl-menuwrapper --> <!-- Header --> <header class="header" role="banner"> <div class="wrapper animated fadeIn"> <div class="content"> <div class="post-title "> <h1>[NLP] BERT란 무엇인가?</h1> <h4>04 Jan 2019</h4> <p class="reading-time"> <i class="fa fa-clock-o"></i> Reading time ~3 minutes </p><!-- /.entry-reading-time --> <a class="btn zoombtn" href="https://moongmoong.github.io/posts/"> <i class="fa fa-chevron-left"></i> </a> </div> <h2 id="bert에-대해-알아보자">BERT에 대해 알아보자</h2> <p>최근 NLP분야에서 가장 압도적인 성능을 보이는</p> <p><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a>.(이하 버트, Bidirectional Encoder Representations from Transformers)</p> <p>2018년 11월에 구글이 공개한 모델로 NLP 전 분야에서 State of the art를 달성하려하는 괴물 모델… 오늘은 이에 대해서 정리하고 마무리를 지으려 한다.</p> <h3 id="bert의-핵심-키워드">BERT의 핵심 키워드</h3> <p>BERT는 사실 새로운 패러다임을 제시한 논문은 아니다. 최근 알려진 높은 성능을 갖는 기술들의 종합이라는 것이 핵심이다.</p> <ol> <li>트랜스퍼 러닝(transfer learning)</li> <li>Transformer 모델 적용</li> </ol> <p>간단하게 위에 두가지를 핵심으로 볼 수 있는데 각각에 대해서 이야기 해보자</p> <h3 id="트랜스퍼-러닝transfer-learning--semi-supervised-learning">트랜스퍼 러닝(transfer learning) / Semi-supervised learning</h3> <p>딥러닝 공부를 하다보면 절대로 피할수 없는 문제가 있다. 바로 <strong>데이터 부족</strong> 문제인데, 막대한 데이터가 필요한 현실과 데이터를 만들 수 없는 현실이 참 아이러니하다.</p> <p>BERT뿐 아니라 최근 높은 성능을 갖는 NLP 모델들은 이를 극복하기 위해서 <strong>트랜스퍼 러닝</strong>을 적용한다.</p> <p>NLP분야에서 NER(Named-entity recognition)이든 QA(Question &amp; Answer)이든 상관없이 공통적으로 <strong>언어</strong>가 갖는 특징이 있다.</p> <p>BERT는 pre-training이라는 이름으로 해당 task에 맞는 데이터가 아닌 인터넷에 방대하게 퍼져있는 (wikipedia, google 등)의 문서들을 활용해서 언어모델(Language model)을 학습시킨다.</p> <p>이렇게 pre-training된 모델을 다시 load해서 해당 task에 맞는 데이터에 추가 훈련을 시켜주는 것을 트랜스퍼 러닝이라 하고 이를 통해서 데이터 부족의 한계를 상당부분 극복할 수 있다.</p> <p>Semi-supervised learning이라 불리는 이유는 BERT가 최종적으로 pre-training에서 un-labeled된 위키피디아와 같은 데이터를 사용하고, post-training에서 Squard1.0과 같은 labeled된 데이터를 사용하여 unsupervised laerning과 supervised learning이 모두 적용되었기 때문이다.</p> <p><strong>pre-training</strong></p> <p>이 부분이 어떻게 수행되는가는 다음주쯤에 업데이트 하도록 하겠습니다.</p> <h3 id="transformer-모델-적용-end-to-end">Transformer 모델 적용 End-to-end</h3> <p>BERT가 사용하는 트랜스포머(Transformer)는 <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">Attention is all you need</a>. 라는 논문에서 제안한 모델 구조이다.</p> <p>2017년 가장 핫했던 모델인 만큼 기본성능이 아주 높다. 그럼 이제 왜 이 모델이 성능이 높은지 알아보도록 하자.</p> <p>기본적으로 BERT와 비교되야할 기술들은 다음과 같다.</p> <ol> <li>word2vec</li> <li>OpenAI GPT</li> <li>ELMO</li> </ol> <p>이 세가지 기술들은 기본적으로 word embedding 테크닉의 일종이다.</p> <p>BERT이전의 NLP task 모델은 위의 세가지 기술과 같은 word embedding 알고리즘에서 나온 vector를 입력으로 받는 main 모델을 따로 구성한다.</p> <p>반면에 BERT는 End-to-end로 추가적인 모델 구성없이 task까지 수행하는 모델 인데.. 사실 이 부분은 그리 중요하지 않다.</p> <p>3가지 기술과 BERT가 기본적으로 ‘Word들의 갖는 뜻’, ‘Word 들이 문장 내에서 다른 단어들과의 상관관계’에 집중하는 것으로 성능을 확보했다는 공통점이 있는 것이 핵심이다.</p> <p>각 기술들에 대해서도 차차 글을 작성하겠지만.. 이번에는 핵심적인 부분만 짚고 넘어가도록 하겠다.</p> <p>논문에 삽입된 이미지를 참고하면 좋을 것 같아서 삽입해 두었다.</p> <p><img src="https://user-images.githubusercontent.com/26134358/50693610-cd6c6b80-107a-11e9-8b0b-1c2a7f863460.png" alt="2019-01-04 23-44-44" /></p> <ol> <li>word2vec</li> </ol> <ul> <li>기본적으로 string인 word를 특정 dim을 갖는 float vector로 매핑하는 기술이다</li> <li>비슷한 단어들은 서로간의 벡터 거리가 가깝도록, 관련이 없는 단어들은 벡터 거리가 멀어지도록 embedding할 수 있게 훈련한다.</li> <li>오로지 single word만을 입력받기 때문에 문장 내에서 , <strong>뜻에 변화를 줄 수 있는 연관된 단어들의 정보를 사용불가</strong></li> </ul> <ol> <li>OpenAI GPT <ul> <li>연속된 word의 집합인 문장을 입력받고, 각 word의 embedding vector를 출력한다.</li> <li>문맥정보(Contextual information)라 불리는 인접한 단어들의 정보를 활용하기 위해서 이전 word들의 embedding 과정에서 생성된 및 hidden layer의 출력들을 다음 word를 embedding할 때 입력으로 같이 사용하게된다.</li> <li>문맥정보를 활용는 시도로 성능을 높일 수 있었지만 다음과 같은 두가지 한계점을 갖는다.</li> </ul> <ul> <li>문장의 <strong>초기 단어들은 문맥정보를 활용불가</strong></li> <li>긴 문장의 경우 <strong>후반부 단어들은 너무 막대한 단어들의 정보가 입력</strong>되고, 어느 단어가 중요한지 알 수 없다.</li> </ul> </li> <li>ELMO <ul> <li>ELMO같은 경우는 따로 한번 더 글을 작성할 계획이 있다.</li> <li>OpenAI GPT가 갖는 한계점 중 한가지를 극복하기 위해서 <strong>forward-LSTM, backword-LSTM</strong>을 동시에 사용한다.</li> <li>두개의 LSTM을 구성하는것으로 <strong>초반부의 단어들도 뒷 부분의 단어들의 특징을 반영</strong>할 수 있다.</li> <li>하지만 주변의 단어중 <strong>어느 단어가 중요한지 알 수 없는점</strong></li> <li>LSTM cell이 갖는 <strong>먼 단어에 대한 정보 소실</strong>과 그 정도조차 알 수 없는점</li> <li>RNN 레이어의 고질적인 문제인 <strong>병렬 연산의 비효율</strong>이 한계로 제시된다.</li> </ul> </li> <li>BERT(Transformer) <ul> <li>Attention을 적용시키는 것으로 전체 문장의 모든 단어를 동시에 입력받고 중요한 단어가 어떤 단어인지 Attention module을 통해서 찾을 수 있도록 훈련</li> <li>어떤 단어를 주목하고 있는지 확인할 수 있다.</li> <li>아주 먼 단어와의 상관관계까지 고려할 수 있다.</li> <li>단점 : 엄청난 크기의 model size로 인한 computation cost</li> </ul> <ul> <li><strong>이 단점을 구글이 TPU를 멀티로 사용하는것으로 해결함..</strong> 구글이 친절하게 pre-training된 savefile을 공개해서 우리는 post-training만 하면 되어서 아주 빠른 훈련이 가능</li> <li>Large model, basic model이 있는데 Large model은 영어만 지원하며, basic model은 다양한 언어를 지원한다</li> <li>최근 한국어 데이터세트를 <a href="https://korquad.github.io/">KorQuAD</a>.가 LG CNS가 공개하였는데, 클로바에서 BERT-basic을 적용하여 F1 score 87.85를 달성할 수 있음을 보여주었다.</li> <li>Google 및 LG CNS 연구원님들에게 감사인사를…</li> </ul> </li> </ol> <div class="entry-meta"> <br> <hr> <span class="entry-tags"><a href="https://moongmoong.github.io/tags/#NLP" title="Pages tagged NLP" class="tag"><span class="term">NLP</span></a><a href="https://moongmoong.github.io/tags/#BERT" title="Pages tagged BERT" class="tag"><span class="term">BERT</span></a><a href="https://moongmoong.github.io/tags/#Transfer-learning" title="Pages tagged Transfer-learning" class="tag"><span class="term">Transfer-learning</span></a><a href="https://moongmoong.github.io/tags/#Transformer" title="Pages tagged Transformer" class="tag"><span class="term">Transformer</span></a></span> <span class="entry-categories"><a href="https://moongmoong.github.io/categories/#NLP" title="Pages categoried NLP" class="category"><span class="term">NLP</span></a></span> <span class="social-share"> <a href="https://www.facebook.com/sharer/sharer.php?u=https://moongmoong.github.io/Bert/" title="Share on Facebook" class="tag"> <span class="term"><i class="fa fa-facebook-square"></i> Share</span> </a> <a href="https://twitter.com/intent/tweet?text=https://moongmoong.github.io/Bert/" title="Share on Twitter" class="tag"> <span class="term"><i class="fa fa-twitter-square"></i> Tweet</span> </a> <a href="https://plus.google.com/share?url=https://moongmoong.github.io/Bert/" title="Share on Google+" class="tag"> <span class="term"><i class="fa fa-google-plus-square"></i> +1</span> </a> </span> <div style="clear:both"></div> </div> </div> </div> </header> <!-- JS --> <script src="https://moongmoong.github.io/assets/js/jquery-1.12.0.min.js"></script> <script src="https://moongmoong.github.io/assets/js/jquery.dlmenu.min.js"></script> <script src="https://moongmoong.github.io/assets/js/jquery.goup.min.js"></script> <script src="https://moongmoong.github.io/assets/js/jquery.magnific-popup.min.js"></script> <script src="https://moongmoong.github.io/assets/js/jquery.fitvid.min.js"></script> <script src="https://moongmoong.github.io/assets/js/scripts.js"></script> <script type="text/javascript"> var disqus_shortname = 'taylantatli'; (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); (document.getElementsByCategoryName('head')[0] || document.getElementsByCategoryName('body')[0]).appendChild(dsq); })(); (function () { var s = document.createElement('script'); s.async = true; s.type = 'text/javascript'; s.src = '//' + disqus_shortname + '.disqus.com/count.js'; (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s); (document.getElementsByCategoryName('HEAD')[0] || document.getElementsByCategoryName('BODY')[0]).appendChild(s); }()); </script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript> <!-- MathJax --> <script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </body> </html>
