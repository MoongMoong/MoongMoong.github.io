---
title: "[NLP] BERT란 무엇인가? (2)"
layout: post
date: '2019-01-08 11:33:00'
excerpt: BERT에 대한 개념적인 설명
comments: true
categories:
- NLP
tag:
- NLP
- BERT
- Transfer-learning
- Transformer
---

## BERT에 대해 알아보자

최근 NLP분야에서 가장 압도적인 성능을 보이는  

[BERT](https://arxiv.org/pdf/1810.04805.pdf).(이하 버트, Bidirectional Encoder Representations from Transformers)  

2018년 11월에 구글이 공개한 모델로 NLP 전 분야에서 State of the art를 달성하려하는 괴물 모델... 오늘은 이에 대해서 정리하고 마무리를 지으려 한다.  

라고 하여 지난 번에 글을 작성하기 시작하여 BERT의 핵심 키워드를 정리했었다.  
이번에는 BERT을 훈련시키는 방법(pre-training/finetuning)에 대해 심도있게 살펴보려 한다.  

### pre-training에 대해 알아보자

Q&A, 문서요약, 핵심어 검출, 음성인식, 화자인식, 음성 합성, 이미지 분류 등  
딥러닝으로 수행되는 수 많은 task가 있다.  
딥러닝의 본질은 '빅 데이터 처리 기법'이다.  
글자 그대로 아주 많은 데이터를 처리하는 것이고, 이들의 패턴을 막대한 양의 연산량을 동원해서 알아내는 것이다.  
그럼 당연이 이를 수행할 수 있을 만큼의 거대한 데이터 세트를 수집해야 하는데, 앞에서 언급했던 task는 가장 유명한 task이면서 '(입력 데이터, 원하는 결과)의 pair가 필요한 supervised 분야'이다.  
그리고 이를 수행하기 위해 'labeled data'를 수집하는 것은 아주 어려운 일이며, 직접 만드는 것은 아주 큰 비용을 필요로한다.  

해결 방법으로 제시된 것이 transfer-learning이다.  



### fine-tuning에 대해 알아보자




