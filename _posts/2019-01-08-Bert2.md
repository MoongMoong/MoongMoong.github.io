---
title: "[NLP] BERT란 무엇인가? (2)"
layout: post
date: '2019-01-08 11:33:00'
excerpt: BERT에 대한 개념적인 설명
comments: true
categories:
- NLP
tag:
- NLP
- BERT
- Transfer-learning
- Transformer
---

## BERT에 대해 알아보자

최근 NLP분야에서 가장 압도적인 성능을 보이는  

[BERT](https://arxiv.org/pdf/1810.04805.pdf).(이하 버트, Bidirectional Encoder Representations from Transformers)  

2018년 11월에 구글이 공개한 모델로 NLP 전 분야에서 State of the art를 달성하려하는 괴물 모델... 오늘은 이에 대해서 정리하고 마무리를 지으려 한다.  

라고 하여 지난 번에 글을 작성하기 시작하여 BERT의 핵심 키워드를 정리했었다.  
이번에는 BERT을 훈련시키는 방법(pre-training/finetuning)에 대해 심도있게 살펴보려 한다.  

### pre-training에 대해 알아보자

Q&A, 문서요약, 핵심어 검출, 음성인식, 화자인식, 음성 합성, 이미지 분류 등  
딥러닝으로 수행되는 수 많은 task가 있다.  
딥러닝의 본질은 '빅 데이터 처리 기법'이다.  
글자 그대로 아주 많은 데이터를 처리하는 것이고, 이들의 패턴을 막대한 양의 연산량을 동원해서 알아내는 것이다.  
그럼 당연이 이를 수행할 수 있을 만큼의 거대한 데이터 세트를 수집해야 하는데, 앞에서 언급했던 task는 가장 유명한 task이면서 '(입력 데이터, 원하는 결과)의 pair가 필요한 supervised 분야'이다.  
그리고 이를 수행하기 위해 'labeled data'를 수집하는 것은 아주 어려운 일이며, 직접 만드는 것은 아주 큰 비용을 필요로한다.  

해결 방법으로 제시된 것이 transfer-learning이다.  



### fine-tuning에 대해 알아보
자
딥러닝 공부를 하다보면 절대로 피할수 없는 문제가 있다.
바로 **데이터 부족** 문제인데, 막대한 데이터가 필요한 현실과 데이터를 만들 수 없는 현실이 참 아이러니하다.

BERT뿐 아니라 최근 높은 성능을 갖는 NLP 모델들은 이를 극복하기 위해서 **트랜스퍼 러닝**을 적용한다.

NLP분야에서 NER(Named-entity recognition)이든 QA(Question & Answer)이든 상관없이 공통적으로 **언어**가 갖는 특징이 있다. 

BERT는 'pre-training'이라는 훈련을 통하여 해당 task에 맞는 데이터가 아닌 인터넷에 방대하게 퍼져있는 (wikipedia, google 등)의 문서들을 활용해서 언어모델(Language model)을 학습시킨다.

이렇게 pre-training된 모델을 다시 load해서 해당 task에 맞는 데이터에 'fine tuning training'을 수행하는 것을 트랜스퍼 러닝이라 하고 이를 통해서 데이터 부족의 한계를 상당부분 극복할 수 있다.

Semi-supervised learning이라 불리는 이유는 BERT가 최종적으로 pre-training에서 un-labeled된 위키피디아와 같은 데이터를 사용하고, post-training에서 Squard1.0과 같은 labeled된 데이터를 사용하여 unsupervised laerning과 supervised learning이 모두 적용되었기 때문이다.

**pre-training**

이 부분이 어떻게 수행되는가는 분량이 좀 많아서 다음주쯤에 다른 글을 통해 업데이트 하도록 하겠습니다.

### Transformer 모델 적용 End-to-end

BERT가 사용하는 트랜스포머(Transformer)는 [Attention is all you need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf). 라는 논문에서 제안한 모델 구조이다.

2017년 가장 핫했던 모델인 만큼 기본성능이 아주 높다.
그럼 이제 왜 이 모델이 성능이 높은지 알아보도록 하자.

기본적으로 BERT와 비교되야할 기술들은 다음과 같다. 

1. word2vec
2. OpenAI GPT
3. ELMO

이 세가지 기술들은 기본적으로 word embedding 테크닉의 일종이다. 

BERT이전의 NLP task 모델은 위의 세가지 기술과 같은 word embedding 알고리즘에서 나온 vector를 입력으로 받는 main 모델을 따로 구성한다. 

반면에 BERT는 End-to-end로 추가적인 모델 구성없이 task까지 수행하는 모델 인데.. 사실 이 부분은 그리 중요하지 않다. 

3가지 기술과 BERT가 기본적으로 'Word들의 갖는 뜻', 'Word 들이 문장 내에서 다른 단어들과의 상관관계'에 집중하는 것으로 성능을 확보했다는 공통점이 있는 것이 핵심이다.

각 기술들에 대해서도 차차 글을 작성하겠지만.. 이번에는 핵심적인 부분만 짚고 넘어가도록 하겠다.

논문에 삽입된 이미지를 참고하면 좋을 것 같아서 삽입해 두었다.

![2019-01-04 23-44-44](https://user-images.githubusercontent.com/26134358/50693610-cd6c6b80-107a-11e9-8b0b-1c2a7f863460.png)


1. word2vec

- 기본적으로 string인 word를 특정 dim을 갖는 float vector로 매핑하는 기술이다
- 비슷한 단어들은 서로간의 벡터 거리가 가깝도록, 관련이 없는 단어들은 벡터 거리가 멀어지도록 embedding할 수 있게 훈련한다.
- 오로지 single word만을 입력받기 때문에 문장 내에서 , **뜻에 변화를 줄 수 있는 연관된 단어들의 정보를 사용불가**

2. OpenAI GPT
- 연속된 word의 집합인 문장을 입력받고, 각 word의 embedding vector를 출력한다.
- 문맥정보(Contextual information)라 불리는 인접한 단어들의 정보를 활용하기 위해서 이전 word들의 embedding 과정에서 생성된 및 hidden layer의 출력들을 다음 word를 embedding할 때 입력으로 같이 사용하게된다.
-   문맥정보를 활용는 시도로 성능을 높일 수 있었지만 다음과 같은 두가지 한계점을 갖는다.
	-   문장의 **초기 단어들은 문맥정보를 활용불가**
	-   긴 문장의 경우 **후반부 단어들은 너무 막대한 단어들의 정보가 입력**되고, 어느 단어가 중요한지 알 수 없다.

3. ELMO
- ELMO같은 경우는 따로 한번 더 글을 작성할 계획이 있다.
- OpenAI GPT가 갖는 한계점 중 한가지를 극복하기 위해서 **forward-LSTM, backword-LSTM**을 동시에 사용한다.
- 두개의 LSTM을 구성하는것으로 **초반부의 단어들도 뒷 부분의 단어들의 특징을 반영**할 수 있다.
- 하지만 주변의 단어중 **어느 단어가 중요한지 알 수 없는점**
-  LSTM cell이 갖는 **먼 단어에 대한 정보 소실**과 그 정도조차 알 수 없는점
-  RNN 레이어의 고질적인 문제인 **병렬 연산의 비효율**이 한계로 제시된다.

4. BERT(Transformer)
- Attention 기반의 Transformer(Attention all you need)를 바탕으로 구성.
- Transformer는 또 다른 글에서 설명하도록 하겠습니다.
- 'Attention의 특징'이 반영되어 전체 문장의 모든 단어를 동시에 입력받고 중요한 단어가 어떤 단어인지 Attention module을 통해서 찾을 수 있도록 훈련
- 어떤 단어를 주목하고 있는지 확인할 수 있다.
- 아주 먼 단어와의 상관관계까지 고려할 수 있다.
- 단점 : 엄청난 크기의 model size로 인한 computation cost
	- **이 단점을 구글이 TPU를 멀티로 사용하는것으로 해결함..** 구글이 친절하게 pre-training된 savefile을 공개해서 우리는 post-training만 하면 되어서 아주 빠른 훈련이 가능
	- Large model, basic model이 있는데 Large model은 영어만 지원하며, basic model은 다양한 언어를 지원한다
	- 최근 한국어 데이터세트를 [KorQuAD](https://korquad.github.io/).가 LG CNS가 공개하였는데, 클로바에서 BERT-basic을 적용하여 F1 score 87.85를 달성할 수 있음을 보여주었다.
	- Google 및 LG CNS 연구원님들에게 감사인사를...




